{% extends 'layouts/base_test.html' %}




{% block body %}
<br>
<br>
<br>
<h2>Oversampling and Data Imbalance</h2>
<hr/>
  <p>
In this section we will explain oversampling as a solution to our recurrent problem of dataset imbalance. As stated in previous documentation sections we have an imbalanced dataset which will bias our
predictions towards the majority class. In our case the majority class is the amount of shoppers who will not purchase anything. Below depicts this behaviour for your e-commerce site.
  </p>
  <img class="img-fluid rounded" src="{{ url_for('static', filename='images/data_imb.png') }}"
   width="600" height="800" alt="">
  <hr/>

  <h3>Oversampling Solution</h3>
  <p>
  Below we can observe that we have a dataset imbalance as only 15.5% of our data represents our positive class
      this will make it so our data does not predict our positive class well which is one aspect we are trying to maximize. The classic oversampling solution is to randomly sample from
      the minority distribution and equalize both quantities for each class. This improves our results in our model.
  </p>
  <hr/>
  <img class="img-fluid rounded" src="{{ url_for('static', filename='images/over_samp.png') }}"
   width="1000" height="1200" alt="">
   <br>
   <h3>SMOTE Oversampling</h3>
   <p>
   <a href="https://www.jair.org/index.php/jair/article/view/10302">SMOTE</a> stands for synthetic minority over-sampling technique. This was a technique introduced by N.V. Chawla which introduces another layer to our
   oversampling. We still randomly sample from our dataset, but now we synthesize observations creating linear distances between each one of them and sampling a new point among its dimensions. This behavior is depicted as follows:


   </p>
   <img class="img-fluid rounded" src="{{ url_for('static', filename='images/smote_ex.png') }}"
    width="1000" height="1200" alt="">
    <br>

<h2>Hyperparameter Tuning</h2>


<p>
  Every model needs to be tuned to certain parameters. The example given is how we can create a grid of parameters which need to be iterated over and tried multiple times.
  Here we will review the two Hyperparameter-tuning techniques used in order to obtain a better predictive model.

<ol>
  <li>Grid Search Cross Validation</li>
  <li>Randomized Search Cross Validation</li>
</ol>

</p>
  <hr/>

  <h3>Grid Search CV</h3>
  <p>Grid Search CV is a function which we can use to pass parameter grids for different models and it will iterate over every single possible combination.</p>
  <p><strong>Grid Search tends to yield better performance for models.</strong></p>
  <br>
  <img class="img-fluid rounded" src="{{ url_for('static', filename='images/grid_cv.png') }}"
   width="1000" height="1200" alt="">
   <br>

  <hr/>
  <h3>Randomized Search CV</h3>
  <p>Randomized Search CV randomly samples from the wide range distribution of parameters and will run iterations 'n' times. We used this method on most of our final models as we did not have
  the computational power in order to fully iterate through every possible outcome.</p>

  <p><strong>Randomized Search is faster for finding out key parameters.</strong></p>

  <img class="img-fluid rounded" src="{{ url_for('static', filename='images/rand_cv.png') }}"
   width="1000" height="1200" alt="">
   <br>
   <br>
   <br>
   <hr/>
{% endblock %}
